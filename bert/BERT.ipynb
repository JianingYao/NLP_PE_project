{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kbn6VoQw7iip",
        "outputId": "b82f3013-2b26-4783-f4f9-fef0ebbca294"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pytorch_pretrained_bert in /usr/local/lib/python3.10/dist-packages (0.6.2)\n",
            "Requirement already satisfied: pytorch-nlp in /usr/local/lib/python3.10/dist-packages (0.5.0)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from pytorch_pretrained_bert) (2.2.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pytorch_pretrained_bert) (1.25.2)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.10/dist-packages (from pytorch_pretrained_bert) (1.34.103)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from pytorch_pretrained_bert) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from pytorch_pretrained_bert) (4.66.4)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from pytorch_pretrained_bert) (2023.12.25)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=0.4.1->pytorch_pretrained_bert) (12.4.127)\n",
            "Requirement already satisfied: botocore<1.35.0,>=1.34.103 in /usr/local/lib/python3.10/dist-packages (from boto3->pytorch_pretrained_bert) (1.34.103)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from boto3->pytorch_pretrained_bert) (1.0.1)\n",
            "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from boto3->pytorch_pretrained_bert) (0.10.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch_pretrained_bert) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch_pretrained_bert) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch_pretrained_bert) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch_pretrained_bert) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.35.0,>=1.34.103->boto3->pytorch_pretrained_bert) (2.8.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=0.4.1->pytorch_pretrained_bert) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=0.4.1->pytorch_pretrained_bert) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.35.0,>=1.34.103->boto3->pytorch_pretrained_bert) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install pytorch_pretrained_bert pytorch-nlp"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jj90J9zM8FQL",
        "outputId": "78203ee9-cd78-475b-a331-3f3a3b42450b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.40.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.14.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import numpy as np\n",
        "import random as rn\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "from transformers import BertModel\n",
        "from transformers import BertTokenizer\n",
        "from transformers import BertConfig\n",
        "\n",
        "\n",
        "from torchnlp.datasets import imdb_dataset\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from torch.optim import Adam\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "from IPython.display import clear_output\n"
      ],
      "metadata": {
        "id": "ZAsQaIFV8Ka_"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rn.seed(321)\n",
        "np.random.seed(321)\n",
        "torch.manual_seed(321)\n",
        "torch.cuda.manual_seed(321)"
      ],
      "metadata": {
        "id": "yVGi_8rx8RkA"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare data"
      ],
      "metadata": {
        "id": "o-wxS3jt8WOd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, test_data = imdb_dataset(train=True, test=True)\n",
        "rn.shuffle(train_data)\n",
        "rn.shuffle(test_data)\n",
        "train_data = train_data[:1000]\n",
        "test_data = test_data[:100]"
      ],
      "metadata": {
        "id": "8ro_hytq8UiK"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_texts, train_labels = list(zip(*map(lambda d: (d['text'], d['sentiment']), train_data)))\n",
        "test_texts, test_labels = list(zip(*map(lambda d: (d['text'], d['sentiment']), test_data)))\n",
        "\n",
        "len(train_texts), len(train_labels), len(test_texts), len(test_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xyqAKXI8h3t",
        "outputId": "bbb4c044-f22f-4ef8-e43c-09a3f2ee4123"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1000, 1000, 100, 100)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9KeuzbOA8yXt",
        "outputId": "82a6d866-923c-41b3-d50a-2ba297dcf220"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.tokenize('Hi my name is Dima')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dP5UjB6j82Jf",
        "outputId": "7cfce490-0140-4713-a739-6a9b54d27eec"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['hi', 'my', 'name', 'is', 'dim', '##a']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_train = tokenizer(train_texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "batch_test = tokenizer(test_texts, padding=True, truncation=True, return_tensors=\"pt\")"
      ],
      "metadata": {
        "id": "hqAUG7Sw85Rc"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_tokens_ids = batch_train['input_ids']\n",
        "test_tokens_ids = batch_test['input_ids']"
      ],
      "metadata": {
        "id": "LqsI3usl9GOb"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_masks = batch_train['attention_mask']\n",
        "test_masks = batch_test['attention_mask']"
      ],
      "metadata": {
        "id": "1eJmJL499I7T"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_y = np.array(train_labels) == 'pos'\n",
        "test_y = np.array(test_labels) == 'pos'\n",
        "train_y.shape, test_y.shape, np.mean(train_y), np.mean(test_y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_oPq9ja9MxH",
        "outputId": "f75abe59-5c90-48cf-f3f5-6e13f370a7b4"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1000,), (100,), 0.489, 0.5)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Baseline"
      ],
      "metadata": {
        "id": "KVxLI3un9W8p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.metrics import classification_report"
      ],
      "metadata": {
        "id": "M5Q7zZV69VJk"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "baseline_model = make_pipeline(CountVectorizer(ngram_range=(1,3)), LogisticRegression()).fit(train_texts, train_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KVcA1O3i9bRu",
        "outputId": "6010199a-30d5-48b7-fbcd-53085309cc9c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "baseline_predicted = baseline_model.predict(test_texts)"
      ],
      "metadata": {
        "id": "_BWIO5oq9pEN"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(test_labels, baseline_predicted))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3cfNFyrf9td7",
        "outputId": "da436af7-1742-4539-e960-e688237889b5"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         neg       0.81      0.86      0.83        50\n",
            "         pos       0.85      0.80      0.82        50\n",
            "\n",
            "    accuracy                           0.83       100\n",
            "   macro avg       0.83      0.83      0.83       100\n",
            "weighted avg       0.83      0.83      0.83       100\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "BERT model"
      ],
      "metadata": {
        "id": "bx4kEHj19xRz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BertBinaryClassifier(nn.Module):\n",
        "    def __init__(self, dropout=0.1):\n",
        "        super(BertBinaryClassifier, self).__init__()\n",
        "\n",
        "        #model_config = BertConfig.from_pretrained('bert-base-uncased', output_hidden_states=True)\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear = nn.Linear(768, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, tokens, masks=None):\n",
        "        outputs = self.bert(tokens, attention_mask=masks)\n",
        "        pooled_output = outputs.pooler_output  # This is the correct way to access the pooled output with newer versions.\n",
        "        dropout_output = self.dropout(pooled_output)\n",
        "        linear_output = self.linear(dropout_output)\n",
        "        proba = self.sigmoid(linear_output)\n",
        "        return proba\n"
      ],
      "metadata": {
        "id": "coGiyuUp9vH8"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BertWithSinusoidalPositionalEmbedding(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.bert = BertModel(config)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.classifier = nn.Linear(config.hidden_size, 1)\n",
        "        self.sigmoid = nn.Sigmoid()  # Adding a sigmoid layer\n",
        "        self.position_embeddings = nn.Parameter(self.sinusoidal_embeddings(512, config.hidden_size), requires_grad=False)\n",
        "\n",
        "    def sinusoidal_embeddings(self, num_positions, hidden_size):\n",
        "        position = torch.arange(num_positions).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, hidden_size, 2) * -(math.log(10000.0) / hidden_size))\n",
        "        sinusoidal = torch.zeros(num_positions, hidden_size)\n",
        "        sinusoidal[:, 0::2] = torch.sin(position * div_term)\n",
        "        sinusoidal[:, 1::2] = torch.cos(position * div_term)\n",
        "        return sinusoidal\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None):\n",
        "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
        "        sequence_output = outputs.last_hidden_state\n",
        "        sequence_output += self.position_embeddings[:sequence_output.size(1), :]\n",
        "        pooled_output = sequence_output[:, 0]\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.classifier(pooled_output)\n",
        "        probabilities = self.sigmoid(logits)  # Apply sigmoid to convert logits to probabilities\n",
        "        return probabilities\n",
        "\n"
      ],
      "metadata": {
        "id": "K0eeAA4IM2-n"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class ALiBiPositionalEmbedding(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.w = nn.Parameter(torch.randn(dim // 2) * 0.02)  # initialize weights\n",
        "\n",
        "    def forward(self, length):\n",
        "        positions = torch.arange(length, device=self.w.device).float().unsqueeze(1)\n",
        "        angles = positions * self.w.view(1, -1)\n",
        "        return torch.cat((angles.sin(), angles.cos()), dim=1)\n",
        "\n",
        "class BertWithALiBi(BertBinaryClassifier):\n",
        "    def __init__(self, dropout=0.1):\n",
        "        super().__init__(dropout)\n",
        "        self.alibi_embeddings = ALiBiPositionalEmbedding(768)  # Assuming BERT hidden size is 768\n",
        "\n",
        "    def forward(self, tokens, masks=None):\n",
        "        outputs = self.bert(tokens, attention_mask=masks)\n",
        "\n",
        "        # Apply ALiBi embeddings\n",
        "        seq_length = tokens.size(1)\n",
        "        alibi_emb = self.alibi_embeddings(seq_length)\n",
        "        alibi_emb = alibi_emb.unsqueeze(0).repeat(tokens.size(0), 1, 1)  # Repeat for batch size\n",
        "\n",
        "        # Modify embeddings\n",
        "        inputs_embeds = outputs.last_hidden_state + alibi_emb\n",
        "        outputs = self.bert(inputs_embeds=inputs_embeds, attention_mask=masks)\n",
        "\n",
        "        pooled_output = outputs.pooler_output\n",
        "        dropout_output = self.dropout(pooled_output)\n",
        "        linear_output = self.linear(dropout_output)\n",
        "        proba = self.sigmoid(linear_output)\n",
        "        return proba\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "d5TBzvUfQKVd"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RoPEPositionalEmbedding(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        # Generate frequencies for half the dimensions\n",
        "        inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2).float() / dim))\n",
        "        self.register_buffer(\"inv_freq\", inv_freq)\n",
        "\n",
        "    def forward(self, pos):\n",
        "        # Create the full range sinusoidal pattern\n",
        "        sinusoid_inp = torch.ger(pos, self.inv_freq)\n",
        "        pos_emb = torch.cat((sinusoid_inp.sin(), sinusoid_inp.cos()), dim=-1)\n",
        "        return pos_emb\n",
        "\n",
        "    @staticmethod\n",
        "    def apply_rotary_pos_emb(x, sinusoid_inp):\n",
        "        sinusoid_inp = sinusoid_inp.unsqueeze(0)  # Expand for batch size\n",
        "        sin, cos = sinusoid_inp.split(sinusoid_inp.shape[-1] // 2, dim=-1)\n",
        "        x1 = x[..., 0::2]\n",
        "        x2 = x[..., 1::2]\n",
        "        rotated_x1 = x1 * cos - x2 * sin\n",
        "        rotated_x2 = x2 * cos + x1 * sin\n",
        "        return torch.cat([rotated_x1, rotated_x2], dim=-1)\n",
        "\n",
        "class BertWithRoPE(BertBinaryClassifier):\n",
        "    def __init__(self, dropout=0.1):\n",
        "        super().__init__(dropout)\n",
        "        self.rope_embeddings = RoPEPositionalEmbedding(768)  # Assuming BERT's hidden size is 768\n",
        "\n",
        "    def forward(self, tokens, masks=None):\n",
        "        outputs = self.bert(tokens, attention_mask=masks)\n",
        "        seq_length = tokens.size(1)\n",
        "        position = torch.arange(seq_length, device=tokens.device).float()\n",
        "        rope_emb = self.rope_embeddings(position)\n",
        "\n",
        "        # Apply rotary embeddings\n",
        "        transformed = self.rope_embeddings.apply_rotary_pos_emb(outputs.last_hidden_state, rope_emb)\n",
        "\n",
        "        pooled_output = outputs.pooler_output\n",
        "        dropout_output = self.dropout(pooled_output)\n",
        "        linear_output = self.linear(dropout_output)\n",
        "        proba = self.sigmoid(linear_output)\n",
        "        return proba\n",
        "\n"
      ],
      "metadata": {
        "id": "dGMAe9orRnhx"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HeaSuwOw93Ht",
        "outputId": "f183bfc8-5bef-442e-f273-4d8320ab444f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "str(torch.cuda.memory_allocated(device)/1000000 ) + 'M'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "RVMW3Usw97on",
        "outputId": "8f3135c7-a24c-4255-9095-ed0212ef1ec2"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'0.0M'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bert_clf = BertBinaryClassifier()\n",
        "#config = BertConfig.from_pretrained('bert-base-uncased')\n",
        "#bert_clf = BertWithSinusoidalPositionalEmbedding(config).to(device)\n",
        "#bert_clf = BertWithALiBi()\n",
        "#bert_clf = bert_clf.to(device)\n",
        "#bert_clf = BertWithRoPE().to(device)\n",
        "\n",
        "bert_clf = bert_clf.cuda()"
      ],
      "metadata": {
        "id": "mErd6md89-y5"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "str(torch.cuda.memory_allocated(device)/1000000 ) + 'M'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "d9gZZ674--MT",
        "outputId": "2dc567b2-6456-4410-9e82-96451584c187"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'439.07328M'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#x = torch.tensor(train_tokens_ids[:3]).to(device)\n",
        "#y, pooled = bert_clf.bert(x)\n",
        "#x.shape, y.shape, pooled.shape\n",
        "\n",
        "x = torch.tensor(train_tokens_ids[:3]).to(device)\n",
        "outputs = bert_clf.bert(x)  # Access outputs from the model\n",
        "pooled = outputs.pooler_output  # Access the pooled output\n",
        "y = outputs.last_hidden_state  # Access the sequence of hidden-states at the output of the last layer\n",
        "\n",
        "\n",
        "print(x.shape, y.shape, pooled.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1qSU7uu_A3k",
        "outputId": "0778d1d4-45d0-46e5-9996-31290b8ee69a"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-25-1140c56651ab>:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  x = torch.tensor(train_tokens_ids[:3]).to(device)\n",
            "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 512]) torch.Size([3, 512, 768]) torch.Size([3, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y = bert_clf(x)\n",
        "y.cpu().detach().numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5oONGcJS_KEC",
        "outputId": "077ebbcb-412e-4f95-ec36-f051f48a82b8"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.48952854],\n",
              "       [0.42610326],\n",
              "       [0.43423972]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "str(torch.cuda.memory_allocated(device)/1000000 ) + 'M'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "oeSj7ZIuCXLW",
        "outputId": "1e56190a-0f47-43a4-deb1-b72b93ac181f"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'3185.033728M'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y, x, pooled = None, None, None\n",
        "torch.cuda.empty_cache()\n",
        "str(torch.cuda.memory_allocated(device)/1000000 ) + 'M'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "bCWjz0sDCYnT",
        "outputId": "24915db4-ad14-4438-975c-de50dde7db16"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1816.313344M'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "FINE tune BERT"
      ],
      "metadata": {
        "id": "V5p90eBUCdan"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 8\n",
        "EPOCHS = 10"
      ],
      "metadata": {
        "id": "MGil7ilkCfL0"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_tokens_tensor = torch.tensor(train_tokens_ids)\n",
        "train_y_tensor = torch.tensor(train_y.reshape(-1, 1)).float()\n",
        "\n",
        "test_tokens_tensor = torch.tensor(test_tokens_ids)\n",
        "test_y_tensor = torch.tensor(test_y.reshape(-1, 1)).float()\n",
        "\n",
        "train_masks_tensor = train_masks\n",
        "test_masks_tensor = test_masks\n",
        "\n",
        "str(torch.cuda.memory_allocated(device)/1000000 ) + 'M'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "id": "xamYEVZeChft",
        "outputId": "d434aeca-c905-4dd9-a136-ad0a85b62382"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-30-2ba93f9af6e1>:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_tokens_tensor = torch.tensor(train_tokens_ids)\n",
            "<ipython-input-30-2ba93f9af6e1>:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  test_tokens_tensor = torch.tensor(test_tokens_ids)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1816.313344M'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = TensorDataset(train_tokens_tensor, train_masks_tensor, train_y_tensor)\n",
        "train_sampler = RandomSampler(train_dataset)\n",
        "train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
        "\n",
        "test_dataset = TensorDataset(test_tokens_tensor, test_masks_tensor, test_y_tensor)\n",
        "test_sampler = SequentialSampler(test_dataset)\n",
        "test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=BATCH_SIZE)"
      ],
      "metadata": {
        "id": "rMW4DO79CloF"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "param_optimizer = list(bert_clf.sigmoid.named_parameters())\n",
        "optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]"
      ],
      "metadata": {
        "id": "QeACsC3qCo3I"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = Adam(bert_clf.parameters(), lr=3e-6)"
      ],
      "metadata": {
        "id": "_wEyMFblCsHx"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "X1cpXViTCudd"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch_num in range(EPOCHS):\n",
        "    bert_clf.train()\n",
        "    train_loss = 0\n",
        "    for step_num, batch_data in enumerate(train_dataloader):\n",
        "        token_ids, masks, labels = tuple(t.to(device) for t in batch_data)\n",
        "        #print(str(torch.cuda.memory_allocated(device)/1000000 ) + 'M')\n",
        "        logits = bert_clf(token_ids, masks)\n",
        "\n",
        "        loss_func = nn.BCELoss()\n",
        "\n",
        "        batch_loss = loss_func(logits, labels)\n",
        "        train_loss += batch_loss.item()\n",
        "\n",
        "\n",
        "        bert_clf.zero_grad()\n",
        "        batch_loss.backward()\n",
        "\n",
        "\n",
        "        clip_grad_norm_(parameters=bert_clf.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        #clear_output(wait=True)\n",
        "\n",
        "    print('Epoch: ', epoch_num + 1)\n",
        "    print(\"{0}/{1} train loss: {2} \".format(step_num, len(train_data) / BATCH_SIZE, train_loss / (step_num + 1)))\n",
        "\n",
        "    bert_clf.eval()\n",
        "    bert_predicted = []\n",
        "    all_logits = []\n",
        "    test_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for step_num_e, batch_data in enumerate(test_dataloader):\n",
        "\n",
        "            token_ids, masks, labels = tuple(t.to(device) for t in batch_data)\n",
        "\n",
        "            logits = bert_clf(token_ids, masks)\n",
        "            loss_func = nn.BCELoss()\n",
        "            loss = loss_func(logits, labels)\n",
        "            test_loss += loss.item()\n",
        "            numpy_logits = logits.cpu().detach().numpy()\n",
        "\n",
        "            bert_predicted += list(numpy_logits[:, 0] > 0.5)\n",
        "            all_logits += list(numpy_logits[:, 0])\n",
        "\n",
        "    print(\"{0}/{1} val loss: {2} \".format(step_num_e, len(test_data) / BATCH_SIZE, test_loss / (step_num_e + 1)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6L47VYLlCxCE",
        "outputId": "96e3f39d-0a9b-4847-aa11-964447bacfa7"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:  1\n",
            "124/125.0 train loss: 0.6792915062904358 \n",
            "12/12.5 val loss: 0.6376867523560157 \n",
            "Epoch:  2\n",
            "124/125.0 train loss: 0.5829170470237732 \n",
            "12/12.5 val loss: 0.49626650030796343 \n",
            "Epoch:  3\n",
            "124/125.0 train loss: 0.4149294217824936 \n",
            "12/12.5 val loss: 0.35381848422380596 \n",
            "Epoch:  4\n",
            "124/125.0 train loss: 0.28490878629684446 \n",
            "12/12.5 val loss: 0.30162300627965194 \n",
            "Epoch:  5\n",
            "124/125.0 train loss: 0.20884878569841384 \n",
            "12/12.5 val loss: 0.27722866374712724 \n",
            "Epoch:  6\n",
            "124/125.0 train loss: 0.16598083633184432 \n",
            "12/12.5 val loss: 0.39180204157645887 \n",
            "Epoch:  7\n",
            "124/125.0 train loss: 0.1231381486132741 \n",
            "12/12.5 val loss: 0.3530760524221338 \n",
            "Epoch:  8\n",
            "124/125.0 train loss: 0.09181714333593845 \n",
            "12/12.5 val loss: 0.46327959284043085 \n",
            "Epoch:  9\n",
            "124/125.0 train loss: 0.0682623934186995 \n",
            "12/12.5 val loss: 0.598803683793029 \n",
            "Epoch:  10\n",
            "124/125.0 train loss: 0.06323728488571942 \n",
            "12/12.5 val loss: 0.5894786770670459 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bert_clf.eval()\n",
        "bert_predicted = []\n",
        "all_logits = []\n",
        "with torch.no_grad():\n",
        "    for step_num, batch_data in enumerate(test_dataloader):\n",
        "\n",
        "        token_ids, masks, labels = tuple(t.to(device) for t in batch_data)\n",
        "\n",
        "        logits = bert_clf(token_ids, masks)\n",
        "        loss_func = nn.BCELoss()\n",
        "        loss = loss_func(logits, labels)\n",
        "        numpy_logits = logits.cpu().detach().numpy()\n",
        "\n",
        "        bert_predicted += list(numpy_logits[:, 0] > 0.5)\n",
        "        all_logits += list(numpy_logits[:, 0])"
      ],
      "metadata": {
        "id": "8_RqjzDGGy-l"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.mean(bert_predicted)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NUYMijaaG5we",
        "outputId": "5949764c-662f-45ff-a8e5-8b3255520f0d"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.57"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(test_y, bert_predicted))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xykt-fioG-NW",
        "outputId": "8b88e0ae-4b34-442f-8232-25e3a5916ff9"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       0.93      0.80      0.86        50\n",
            "        True       0.82      0.94      0.88        50\n",
            "\n",
            "    accuracy                           0.87       100\n",
            "   macro avg       0.88      0.87      0.87       100\n",
            "weighted avg       0.88      0.87      0.87       100\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(5):\n",
        "  clear_output(wait=True)\n",
        "  print('a', i+1)\n",
        "  print(\"{0}/{1} loss: {2} \".format(i, i / 5, i / (i + 1)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2D2iVyvGG_aD",
        "outputId": "5f0d8bfb-80d7-4cc0-8e5d-d538ac0b5319"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a 5\n",
            "4/0.8 loss: 0.8 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sinusoidal"
      ],
      "metadata": {
        "id": "b2m8mhNGJXO9"
      }
    }
  ]
}